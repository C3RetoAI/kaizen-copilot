{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikerochoa/opt/anaconda3/envs/kaizen-copilot-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from git import Repo\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_repo_set(jsonl_path):\n",
    "    repos = set()\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                repo = sample.get(\"repo\")\n",
    "                if repo:\n",
    "                    repos.add(repo.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_repos = get_unique_repo_set(\"./data/swe_bench_lite_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commits_by_repo(jsonl_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extrae un diccionario con repositorios como claves y lista de base_commits únicos como valores.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path (str): Ruta al archivo .jsonl con entradas que contienen 'repo' y 'base_commit'.\n",
    "\n",
    "    Returns:\n",
    "        dict: { repo_name: [base_commit1, base_commit2, ...] }\n",
    "    \"\"\"\n",
    "    commits_by_repo = defaultdict(set)  # set para evitar duplicados\n",
    "\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                repo = item.get(\"repo\")\n",
    "                base_commit = item.get(\"base_commit\")\n",
    "                if repo and base_commit:\n",
    "                    commits_by_repo[repo].add(base_commit)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    # Convertimos los sets en listas\n",
    "    return {repo: list(commits) for repo, commits in commits_by_repo.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_commits = extract_commits_by_repo(\"./data/swe_bench_lite_test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ver ejemplo\n",
    "print(repo_commits[\"django/django\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(lite_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_repo_commits = dict(sorted(repo_commits.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['astropy/astropy', 'django/django', 'matplotlib/matplotlib', 'mwaskom/seaborn', 'pallets/flask', 'psf/requests', 'pydata/xarray', 'pylint-dev/pylint', 'pytest-dev/pytest', 'scikit-learn/scikit-learn', 'sphinx-doc/sphinx', 'sympy/sympy'])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(sorted_repo_commits.keys())\n",
    "print(len(sorted_repo_commits.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHIB_BASE_URL = \"https://github.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(root_dir, extensions={\".py\"}):\n",
    "    collected = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        if any(excluded in dirpath for excluded in [\".git\", \"tests\", \"test\", \"node_modules\", \".venv\", \"__pycache__\"]):\n",
    "            continue\n",
    "        for fname in filenames:\n",
    "            if any(fname.endswith(ext) for ext in extensions):\n",
    "                collected.append(os.path.join(dirpath, fname))\n",
    "    return collected\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def embed_repo(repo_path, base_commit):\n",
    "    files = collect_files(repo_path)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    docs = []\n",
    "\n",
    "    for path in files:\n",
    "        content = read_file(path)\n",
    "        if not content.strip():\n",
    "            continue\n",
    "        \n",
    "        path_split = path.split(\"/\")\n",
    "        name = path_split[-1]\n",
    "\n",
    "        content = f\"[CLS] {name}\\n\" + content\n",
    "        embedding = model.encode(content)  \n",
    "        embeddings.append(embedding)\n",
    "        metadata.append({\"name\": name, \"extension\": \"py\", \"path\": \"/\".join(path_split[-2:]), \"base_commit\": base_commit})\n",
    "        docs.append(content)\n",
    "\n",
    "    return embeddings, metadata, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repo(repo, base_commit: str):\n",
    "    # 1. Crear carpeta temporal\n",
    "    repo_url = GITHIB_BASE_URL + repo\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        print(f\"Clonando {repo_url} en {tmp_dir}\")\n",
    "        try:\n",
    "            repo_obj = Repo.clone_from(repo_url, tmp_dir)\n",
    "            repo_obj.git.checkout(base_commit)\n",
    "            print(f\"→ Checkout exitoso al commit {base_commit}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al procesar {repo_url} @ {base_commit}: {e}\")\n",
    "            return [], [], []\n",
    "        # _ = input(\"lll\")\n",
    "        embeddings, metadata, docs = embed_repo(tmp_dir, base_commit)\n",
    "\n",
    "        # 4. Guardar en vector DB\n",
    "        # vector_db.add(embeddings, metadata=metadata)\n",
    "\n",
    "        # 5. tmp_dir se elimina automáticamente\n",
    "        print(f\"Repo {repo_url} procesado y eliminado.\")\n",
    "        return embeddings, metadata, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8005, settings=Settings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1) Repo: astropy/astropy ------------------------ \n",
      "\n",
      "Clonando https://github.com/astropy/astropy en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmplde3ceon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/astropy/astropy procesado y eliminado.\n"
     ]
    }
   ],
   "source": [
    "current_repos = list_repos[:1]\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for idx, repo in enumerate(current_repos):\n",
    "    print(f\"({idx}/{len(current_repos)}) Repo: {repo} ------------------------ \\n\")\n",
    "    embeddings, metadata, docs = process_repo(repo)\n",
    "    collection_name = repo.replace(\"/\", \"_\")\n",
    "    \n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    for i in range(0, len(docs), BATCH_SIZE):\n",
    "        batch_docs = docs[i:i+BATCH_SIZE]\n",
    "        batch_embeddings = embeddings[i:i+BATCH_SIZE]\n",
    "        batch_metadata = metadata[i:i+BATCH_SIZE]\n",
    "        batch_ids = [str(uuid.uuid4()) for _ in batch_docs]\n",
    "\n",
    "        collection.upsert(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_docs,\n",
    "            embeddings=batch_embeddings,\n",
    "            metadatas=batch_metadata\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, repo in enumerate(list_repos):\n",
    "#     collection_name = repo.replace(\"/\", \"_\")\n",
    "#     chroma_client.delete_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = chroma_client.get_collection(name=\"django_django\")\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "['matplotlib_matplotlib', 'pydata_xarray', 'psf_requests', 'scikit-learn_scikit-learn', 'pallets_flask', 'pytest-dev_pytest', 'pylint-dev_pylint', 'sphinx-doc_sphinx', 'django_django', 'sympy_sympy', 'mwaskom_seaborn', 'astropy_astropy', 'C3RetoAI_Testing-Github-Bot']\n"
     ]
    }
   ],
   "source": [
    "collections = chroma_client.list_collections()\n",
    "print(len(collections))\n",
    "print([c.name for c in collections])  # should not include the deleted one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaizen-copilot-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
